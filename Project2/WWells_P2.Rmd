---
title: "DATA 643: Recommender Systems"
author: "Walt Wells, Summer 2017"
subtitle: "Project 2"
output:
  html_document:
    css: ../custom.css
    highlight: zenburn
    theme: lumen
    toc: true
    toc_float: true
---

# Assignment Requirements

_The goal of this assignment is for you to try out different ways of implementing and configuring a recommender, and to evaluate your different approaches._

For assignment 2, start with an existing dataset of user-item ratings, such as our toy books dataset, MovieLens, Jester [http://eigentaste.berkeley.edu/dataset/] or another dataset of your choosing. Implement at least two of these recommendation algorithms:

* Content-Based Filtering
* User-User Collaborative Filtering
* Item-Item Collaborative Filtering

As an example of implementing a Content-Based recommender, you could build item profiles for a subset of MovieLens movies from scraping http://www.imdb.com/ or using the API at https://www.omdbapi.com/ (which has very recently instituted a small monthly fee). A more challenging method would be to pull movie summaries or reviews and apply tf-idf and/or topic modeling.

You should evaluate and compare different approaches, using different algorithms, normalization techniques, similarity methods, neighborhood sizes, etc. You don’t need to be exhaustive—these are just some suggested possibilities.

You may use the course text’s recommenderlab or any other library that you want.
Please provide at least one graph, and a textual summary of your findings and recommendations.

# Environment Prep

```{r, message=F, warning=F}
if (!require('recommenderlab')) install.packages('recommenderlab')
if (!require('ggplot2')) install.packages('ggplot2')
if (!require('XML')) install.packages('XML')
```

# Data Import

For this project we will use a subset of Dataset 2+ of the [Jester dataset](http://eigentaste.berkeley.edu/dataset/). 

This dataset is used courtesy of Dr. Ken Goldberg.  

__Publication Citation:__
Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001. 

To make it easier to import, the .zip file has been downloaded and the resulting .xls converted to a simple csv.  We also utilize the [joke .zip](http://eigentaste.berkeley.edu/dataset/jester_dataset_1_joke_texts.zip) in the final step. 

```{r, cache=TRUE}
jester <- read.csv("jesterfinal151cols.csv", header=F)
jester <- jester[,1:101]# remove last 50 so it matches with our available joke text
Full <- dim(jester)
```

# Subset {.tabset .tabset-pills} 

Our goal here is simply to get a manageable subset of the full dataset, but with a nice sparseness ratio to make it easier to understand and implement the basic modelling concepts. 

## Subset Method 1: User 

Subset by num jokes a user rated.

```{r, message=F, warning=F}
# to keep our utility matrix manageable, we will use only the tiny slice with the number of observations equal to our cutoff
n <- .7 # will keep matrix semi-sparse
cutoff <- quantile(jester$V1, n)[[1]]
temp <- jester[jester$V1 == cutoff,] 

# change 99s (default NA) to NAs
temp[temp==99] <- NA
Sub1 <- dim(temp)
```

## Subset Method 2: Jokes

Subset by removing columns with NO and ALL ratings.

```{r, message=F, warning=F}
# https://stackoverflow.com/questions/11330138/find-columns-with-all-missing-values
allNAs <- sapply(temp, function(x) all(is.na(x)))
#table(allNAs)
temp <- temp[,!allNAs]
noNAs <- sapply(temp, function(x) all(!is.na(x)))
#table(noNAs)
utility <- temp[,!noNAs]
Sub2 <- dim(utility)
```

## Data Reduction Table

```{r, message=F, warning=F}
percentReduction <- round((1 - Sub2/Full) * 100, 0)
review <- t(data.frame(Full, Sub1, Sub2, percentReduction, 
                       row.names=c("Users", "Jokes")))
knitr::kable(review)
```

## Final Subset Review

```{r}
populated <- colSums(!is.na(utility))
quantile(populated)
qplot(populated) + stat_bin(bins = 50) + ggtitle("Distribution of Number of Times a Joke was Ranked in Final Utility Matrix")
```

# Data Preparation

In this section we will take convert our utility matrix into a "realRatingMatrix" object for use with the "recommenderlab" package, do some initial data exploration, and split our data into train and test sets to prepare for modeling.

## RecommenderLab Matrix

```{r}
jesterUtility <- as(as.matrix(utility), "realRatingMatrix")
rm(jester, temp, review) #clean house
```

## Data Exploration

```{r, warning=F, message=F}
mean_rating <- colMeans(jesterUtility, na.rm = T)
quantile(mean_rating)
goodrating <- quantile(mean_rating, .5)
qplot(mean_rating) + ggtitle("Distribution of Joke Rating") + geom_vline(xintercept = goodrating, col='orange')
image(jesterUtility[1:80,], main="Heatmap: First 80 Users v Jokes")
```

# Modeling {.tabset .tabset-pills} 

## K-Fold Cross Validation

We will use leave one out cross validation to compare different models.

```{r}
# use leave one out and k = 10
set.seed(643)
eval_sets <- evaluationScheme(data=jesterUtility, 
                              method = "cross-validation",
                              given = -1,
                              goodRating = goodrating, # >= 50% of the mean ratings
                              k = 10)
eval_sets
getData(eval_sets, "train")
getData(eval_sets, "known")
getData(eval_sets, "unknown")
```

## Comparing Multiple Models

We'll train Collaborative Filtering models with an eye toward 3 factors:  

* Item Based or User Based
* Similarity Measures of cosine or pearson
* Normalized by Z-score

For simplicity, we will consider Content Filtering, Binarization, and neighborhood size  selction out of scope here. 

```{r, warning=F, message=F, results="hide"}
models_to_evaluate <- list(
    IBCF_cos = list(name = "IBCF", param = list(method = "cosine")),
    IBCF_pear = list(name = "IBCF", param = list(method = "pearson")),
    IBCF_cos_Z = list(name = "IBCF", param = list(
        normalize = "Z-score", method="cosine")),
    UBCF_cos = list(name = "UBCF", param = list(method = "cosine")),
    UBCF_pear = list(name = "UBCF", param = list(method = "pearson")),
    UBCF_cos_Z = list(name = "UBCF", param = list(
        normalize = "Z-score", method="cosine")),
    random = list(name = "RANDOM", param =  NULL)
)

n_recommendations <- c(1, 3, 5, 10, 15, 25, 40)

list_results <- evaluate(x = eval_sets, 
                         method = models_to_evaluate, 
                         n = n_recommendations, 
                         type = "topNList")

evalNums <- evaluate(x = eval_sets, 
               method = models_to_evaluate, 
               type="ratings")
```

## ROC | AUC

```{r}
plot(list_results, annotate=1, legend = "bottomright") 
title("ROC curve")
```

## Precision | Recall 

```{r}
plot(list_results, "prec/rec", annotate = 1, legend = "topright")
title("Precision-recall")
```

## Get RMSE for models

```{r}
evalTable <- avg(evalNums)
evalTable <- t(sapply(evalTable, rbind))
colnames(evalTable) <- c("RMSE", "MSE", "MAE")
knitr::kable(evalTable)
```

# Results {.tabset .tabset-pills}

Across all models, User-Based Collaborative filtering that uses cosine similarity and is fit to the normal distribution using Z-scores had high marks in AUC, Precision v Recall, and RMSE.   

Let's get use this model to recommend a single joke to a single user.  

## Single User: Pred + Fav

```{r}
eval_recommender <- Recommender(data=getData(eval_sets, "train"), method = "UBCF", parameter = list(method="cosine", normalize="Z-score"))

# extract prediction for one user using best model
mypredict <- predict(object = eval_recommender, newdata = getData(eval_sets, "known"), type = "topNList", n=1)
recc_user_1 <- mypredict@items[[1]]
user1_topjoke <- mypredict@itemLabels[recc_user_1]
recjokeRating <- mypredict@ratings[[1]]

# what was their other highest rated joke?  
test <- normalize(getData(eval_sets, "known"), method="Z-score")
test <- as(test, 'matrix')
user1_favjoke <- names(which.max(test[1,] ))
favjokeRating <- max(test[1,], na.rm=T)
```

## Get Joke Helper Function

```{r}
getJoke <- function(recnum) {
    # get a recommended joke from the zip file w/o unzipping only in mem
    recnum <- as.numeric(gsub("[A-Z]", "", recnum))
    filename <- paste0('jokes/init', recnum, '.html')
    joke <- readLines(unz('jester_dataset_1_joke_texts.zip', filename))
    # inspired by: http://www.quantumforest.com/2011/10/reading-html-pages-in-r-for-text-processing/
    html <- htmlTreeParse(joke,useInternal = TRUE)
    text <- unlist(xpathApply(html, '//td', xmlValue))
    text <- gsub('\n', ' ', text)
    text <- sub("^[^a-zA-Z]*", "", text)
    text <- gsub('  ', ' ', text)
    text <- gsub("\\\"", "'", text)
    text <- paste(text, collapse = ' ')
}
```

## Show Favorite and Recommended

```{r}
print(paste0("Favorite Joke: ", user1_favjoke, " | Normalized Rating: ", favjokeRating))
print(getJoke(user1_favjoke))

print(paste0("Recommended Joke: ", user1_topjoke, " | Normalized Rating Prediction: ", recjokeRating))
print(getJoke(user1_topjoke))
```

# References

* [Building a Recommendation System with R by Suresh K. Gorakala, Michele Usuelli](https://www.amazon.com/dp/B012O8S1YM/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)
* https://stackoverflow.com/questions/11330138/find-columns-with-all-missing-values
* https://github.com/wwells/CUNY_DATA_643/blob/master/Project1/WWells_P1.Rmd
* https://rpubs.com/jeknov/movieRec
* https://rpubs.com/tarashnot/recommender_comparison
* http://www.infofarm.be/articles/alternating-least-squares-algorithm-recommenderlab
* http://www.quantumforest.com/2011/10/reading-html-pages-in-r-for-text-processing/


