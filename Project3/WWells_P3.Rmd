---
title: "DATA 643: Recommender Systems"
author: "Walt Wells, Summer 2017"
subtitle: "Project 3:  Matrix Factorization"
output:
  html_document:
    css: ../custom.css
    highlight: zenburn
    theme: lumen
    toc: true
    toc_float: true
---

# Assignment

_The goal of this assignment is give you practice working with Matrix Factorization techniques._

Your task is implement a matrix factorization method—such as singular value decomposition (SVD) or Alternating Least Squares (ALS)—in the context of a recommender system.

You may approach this assignment in a number of ways. You are welcome to start with an existing recommender system written by yourself or someone else. Remember as always to cite your sources, so that you can be graded on what you added, not what you found.

SVD can be thought of as a pre-processing step for feature engineering. You might easily start with thousands or millions of items, and use SVD to create a much smaller set of “k” items (e.g. 20 or 70).

## Notes/Limitations

* SVD builds features that may or may not map neatly to items (such as movie genres or news topics). As in many areas of machine learning, the lack of explainability can be an issue).
* SVD requires that there are no missing values. There are various ways to handle this, including (1) imputation of missing values, (2) mean-centering values around 0, or (3) <advanced> using a more advance technique, such as stochastic gradient descent to simulate SVD in populating the factored matrices.
* Calculating the SVD matrices can be computationally expensive, although calculating ratings once the factorization is completed is very fast. You may need to create a subset of your data for SVD calculations to be successfully performed, especially on a machine with a small RAM footprint.

# Environment Prep

```{r, message=F, warning=F}
if (!require('recommenderlab')) install.packages('recommenderlab')
if (!require('ggplot2')) install.packages('ggplot2')
if (!require('XML')) install.packages('XML')
```

# Data Import

For this project we will use a subset of Dataset 2+ of the [Jester dataset](http://eigentaste.berkeley.edu/dataset/). 

This dataset is used courtesy of Dr. Ken Goldberg.  

__Publication Citation:__
Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001. 

To make it easier to import, the .zip file has been downloaded and the resulting .xls converted to a simple csv.  We also utilize the [joke .zip](http://eigentaste.berkeley.edu/dataset/jester_dataset_1_joke_texts.zip) in the final step. 

```{r}
jester <- read.csv("https://raw.githubusercontent.com/wwells/CUNY_DATA_643/master/Project2/jesterfinal151cols.csv", header=F)
jester <- jester[,2:101]# remove last 50 so it matches with our available joke text
dim(jester)
```

# Data Preparation {.tabset .tabset-pills} 

## All NA Col Removal

```{r}
jester[jester==99] <- NA
allNAs <- sapply(jester, function(x) all(is.na(x))) # remove cols with all NAs
table(allNAs)
jester <- jester[,!allNAs] 
#for this exercise we will leave the columns where every user ranked an item 
```

## Center and Scale

```{r}
jesterScaled <- data.frame(scale(jester, 
                                center=T, 
                                scale=T))

#we'll also save our matrix names for later
ItemIndex <- names(jesterScaled)
```

## NA Review:  % Sparseness

```{r}
sum(is.na(jesterScaled))/prod(dim(jesterScaled))
```

## NA Handling - ColMean Imputation

I feel like I need another month (lifetime?!) to study these different methods for imputation, matrix factorization, and in particular, how to handle for sparse systems. 

I investigated and attempted to use the mice package, but it was too computationally expensive.  I made initial forays into non-svd algorithms, but need more time to really understand them.

For now, while it may not be the best solution, we will impute using column means.  

```{r}
jMeans <- colMeans(jesterScaled, na.rm=TRUE)
indx <- which(is.na(jesterScaled), arr.ind=TRUE)
jesterScaled[indx] <- jMeans[indx[,2]]
```

# SVD {.tabset .tabset-pills} 

## Perform SVD | Base Package

```{r}
svdJester <- svd(jesterScaled)

U <- svdJester$u
V <- svdJester$v
S <- svdJester$d
SMat <- diag(S)
```

## Helper Function Feature Selection

```{r}
SVDRemoveTable <- function(s) {
    df <- data.frame()
    for (i in seq(nrow(s))) {
        m <- sum(s[1:i, 1:i]^2)/sum(s^2)
        df <- rbind(df, c(i, m))
    }
    names(df) <- c("concepts", "variability")
    df
}
```

## Table and Plot

```{r}
mytable <- SVDRemoveTable(SMat)
mytable[48:55,]

ggplot(mytable, aes(concepts, variability)) + geom_point() + 
    geom_hline(yintercept = .9, color='red') + 
    ggtitle("Raw Data")
```

## New $U$ $\Sigma'$ and $V$

We can see that we can retain 90% of the variability by keeping 50 of the 90 singular values.   Let's create our new SVD matrices.

```{r}
k <- 50

NewSMat <- SMat[1:k, 1:k]
NewU <- U[,1:k]
NewV <- V[,1:k]
```


# Data Exploration {.tabset .tabset-pills} 

## New M

We won't necessarily need or use this, but we'll reconstruct as exercise and to confirm variability kept and do some basic data summary. 

```{r}
utilityM <- NewU %*% NewSMat %*% t(NewV)
colnames(utilityM) <- ItemIndex

#confirm variability kept
sum(utilityM^2)/sum(jesterScaled^2)
```

## Distribution of Ratings

```{r}
hist(utilityM, main = "Dist of All Rating")
hist(rowMeans(utilityM), main = "Dist of Mean User Rating")
hist(colMeans(utilityM), main = "Dist of Mean Item Rating")
```

## Summary Stats

```{r}
summary(as.vector(utilityM))
rm(indx, U, SMat, V) # CleanHouse
```

# Modeling: Use $U$ and $V$ to Recommend  {.tabset .tabset-pills}

We'll use the decomposed matrix to query a user and make recommendations based on their concept space.  We'll then check their original row in the table to make sure we're picking recommendations they haven't yet rated, and return top n recommendations.

## Get User | Query Point

```{r}
set.seed(643)
samp <- sample(nrow(NewU),1)
q <- NewU[samp,] #get random user from our U
SampleUserActual <- jester[samp,] # get actual user, so we know what they did and didn't really rate
didnotRate <- is.na(SampleUserActual)
```

## Get qV

Our prediction is gained by recomposing the Utility Matrix using our factorized matrix.  We then add those given values to the colMeans of the original scaled matrix (preSVD).  

$$\hat{r}_{ui} = \bar{r}_u + U_{ki} * \Sigma_k * V_k^t$$

```{r}
qV <- colMeans(jesterScaled) + q %*% NewSMat %*% t(NewV)
qV[!didnotRate] <- NA #remove the predictions from the ones already rated
qV <- data.frame(Index = ItemIndex, 
                 Prediction = t(qV))
```

## Get Top N Recommendations

```{r}
n <- 4
sorted_qV <- qV[order(-qV$Prediction), ]
top_rec <- as.character(sorted_qV$Index[1])
head(sorted_qV, n)
```

## Get Previous Favorite

```{r}
userfav <- names(which.max(SampleUserActual))
```

# Results {.tabset .tabset-pills}

## Get Joke Helper Function

```{r}
getJoke <- function(recnum) {
    # get a recommended joke from the zip file w/o unzipping only in mem
    recnum <- as.numeric(gsub("[A-Z]", "", recnum))
    filename <- paste0('jokes/init', recnum, '.html')
    joke <- readLines(unz('../Project2/jester_dataset_1_joke_texts.zip', filename))
    # inspired by: http://www.quantumforest.com/2011/10/reading-html-pages-in-r-for-text-processing/
    html <- htmlTreeParse(joke,useInternal = TRUE)
    text <- unlist(xpathApply(html, '//td', xmlValue))
    text <- gsub('\n', ' ', text)
    text <- sub("^[^a-zA-Z]*", "", text)
    text <- gsub('  ', ' ', text)
    text <- gsub("\\\"", "'", text)
    text <- paste(text, collapse = ' ')
}
```

## Show Favorite and Recommended

```{r}
print(paste0("Favorite Joke: ", userfav))
print(getJoke(userfav))

print(paste0("Recommended Joke: ", top_rec))
print(getJoke(top_rec))
```

# References

* [Building a Recommendation System with R by Suresh K. Gorakala, Michele Usuelli](https://www.amazon.com/dp/B012O8S1YM/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)
* https://github.com/wwells/CUNY_DATA_643/tree/master/Project2
* [Mining of Massive Datasets, Anand Rajaraman and Jeffrey Ullman, Chapter 11.3](http://infolab.stanford.edu/~ullman/mmds/book.pdf)
* [SVD Gives the Best Low Rank Approximation (Advanced) | Stanford (VIDEO)](https://youtu.be/c7e-D2tmRE0?list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV)
* https://stackoverflow.com/questions/25835643/replacing-missing-values-in-r-with-column-mean
* https://stats.stackexchange.com/questions/214728/should-data-be-normalized-before-or-after-imputation-of-missing-data
* http://ijcai13.org/files/tutorial_slides/td3.pdf


